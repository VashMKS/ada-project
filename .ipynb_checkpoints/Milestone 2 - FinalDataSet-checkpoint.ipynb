{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Echo Chamber Effect - A Case Study (provisional title)\n",
    "\n",
    "## Milestone 2\n",
    "\n",
    "In this notebook we will download a small sample of the Reddit dataset for the first time and perform some basic statistics on it. We will talk about the structure of the data, our pipepline to handle it and how this new knowledge has affected our plans for the project.\n",
    "\n",
    "We will also load the two recommended NLP libraries and try them out to see how they work and to which extent we can take advantage of them.\n",
    "\n",
    "The full report can be found on the updated README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init(r'C:\\Spark\\spark-2.3.2-bin-hadoop2.7')\n",
    "#findspark.init('/Users/vikalpkamdar/opt/spark')\n",
    "#findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is big beyond comprehension. Around 2TB of comments from Reddit's inception in 2005 until March 2017. Our machines cannot handle such numbers and even if they could it would take too much time. Instead, we decided to, for now, focus only on 2016 (the year of the US presidential election) which itslef represents already more than 360GB of data and sticked to the following pipeline:\n",
    "\n",
    " - Sample 10% of the data (~12GB compressed)\n",
    " - Try our scripts locally using spark\n",
    " - Once prototyped, upload the scripts to the cluster, run them on the whole year of data and retrieve only the essential parts\n",
    "\n",
    "This way we streamlined the queries to the cluster and created an easily expandable workframe.\n",
    "\n",
    "In order to filter and reduce even more the size of the datasets we dropped the columns that mostly have no use for us, namely: ```author_flair_class```, ```author_flair_text```, ```distinguished```, ```edited```, ```retrieved_on```, ```stickied``` and ```subreddit_id```. (Script used on cluster: `script/drop_col.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't think a dataset as rich as this needs much enrichment, but we could consider using web scrapping techniques to retrieve subredit data (e.g. the rules of the subreddit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the sample to explain the data\n",
    "\n",
    "The following code does not really need the data in its size as it's for explanatory purposes. The queries and operations were performed on our local system with 1% 2016 reddit comments. Once we figured out the querries we applied them to 10% of the 2016 reddit data (~12GB compressed) by sending python scripts (found in scripts/ folder) to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.parquet(DATA_DIR + \"final_dataset_2016_user_sampled_post_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "df_spark.count() # number of comments used for the 1% dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we show the column names of the spark data frame and the first row in order to analyze how the data frame is structured, the differents columns and its meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of 1 comment\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explain the columns one by one:\n",
    "* __'author':__ Redditor username\n",
    "* __'author_flair_css_class':__ CSS class, defines the style of the 'author_flair_text' tag within the html of Reddit.\n",
    "* __'author_flair_text':__  A tag associated to your username, which is only visible in the current subreddit. \n",
    "* __'body':__ The content of the comment.\n",
    "* __'controversiality':__ When its value is 1, the comment has been voted above a certain threshold and has roughly the same amount of downvotes and upvotes. 0 Otherwise. [More info](https://www.reddit.com/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/)\n",
    "* __'created_utc':__ Unix timestamp of the creation date.\n",
    "* __'distinguished':__ If a comment is distinguished, this means that a moderator or admin of the subreddit has highlighted it (A little [M] will appear next to it).\n",
    "* __'edited':__ Indicates if the comment has been edited. If it has not been edited the variable is False, else its value is the Unix Timestamp of the last edition.\n",
    "* __'gilded':__ When its value is 1, that means a [premium](https://www.reddit.com/premium/) user gave the author [reddit gold](https://www.reddit.com/coins/) (a week of reddit premium). \n",
    "* __'id':__ Unique ID of the comment.\n",
    "* __'link_id':__ Id of the link (similar to post) this comment is anwering to. Always begins with 't3_'.\n",
    "* __'parent_id':__ Id of the post this comment is below. Always begins with 't3_'.\n",
    "* __'retrieved_on':__ Unix Timestamp of the comment retrieval (to this data set).\n",
    "* __'score':__ Upvotes minus downvotes (Reddit mildly obfuscates this value when the comment has lots of votes as a security measure against external meddling. This is called [vote fuzzing](https://www.reddit.com/r/firstdayontheinternet/comments/30b44n/could_someone_explain_how_the_reddit_karma_system/).\n",
    "* __'stickied':__ Boolean value. When a comment is stickied (by a moderator or admin), the comment appears as the first entry right below the original post.\n",
    "* __'subreddit':__ Name of the subreddit where the comment was posted.\n",
    "* __'subreddit_id':__ Unique ID (or full name as refered to in the Rddit API) of the subreddit. Note that those always begin with 't5_'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_spark.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the different values of some of the columns (only the relevant ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_spark[['controversiality']].distinct().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark[['distinguished']].distinct().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_spark[['edited']].distinct().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark[['link_id']].distinct().orderBy(desc(\"link_id\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark[['score']].orderBy(asc('score')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of subreddits in our 1% sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_spark[['subreddit_id']].distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of controversial comments in our 1% sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.filter('controversiality = 1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the local processing we will use pandas as it is easier. By the time our data pieces will be filtered and small enough to not run into memory constraints. Using pyarrow we can read parquet files directly into pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install pyarrow, we get an error if we try to transform the spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pd = pd.read_parquet(DATA_DIR + \"final_dataset_2016_user_sampled_post_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now print out some basic properties and stats about our 1% data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pd[['controversiality', 'distinguished', 'edited', 'parent_id']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pd['id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two cells above we can confirm that the `id` is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pd['subreddit'].value_counts().to_frame().hist(bins=[10,100,1000,2000,3000,10000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the histogram above, a lot of the subreddits have only 1 comment. Hence these subreddits will be considered as outliers, as we feel they do will not help us in our eco-chamber calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide a threshold for the number of comments per subreddit. Subreddits below that threshold will not bring us much if not any information in our calculations for milestone 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we show what we can do some of the above operations on a bigger dataset (10% of the 2016 reddit comments). Firstly we will show the functions we will use to obtain simple statistics. In the next part we will export these commands onto a python script (script/stats.py) and run them on the 10% sampled 2016 reddit comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats below are done on the 1% sample of the 2016 reddit comments. But they were just done in order to find the right functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_comments = df_spark.count()\n",
    "total_n_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the number of removed comments (A comment has been removed when its body is \"[deleted]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_deleted_comments = df_spark.filter('body = \"[deleted]\"').count()\n",
    "n_deleted_comments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of comments that are \"removed\" according to the above criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_deleted_comments = 100 * n_deleted_comments  / total_n_comments\n",
    "percentage_deleted_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the number of comments posted by removed users (Users who have removed their account after post the comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_by_removed_users = df_spark.filter('author = \"[deleted]\"').count()\n",
    "comments_by_removed_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of comments that were posted by \"removed\" users according to the above criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percentage_by_dl_user = 100 * comments_by_removed_users / total_n_comments\n",
    "percentage_by_dl_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the total number of controversial comments and the percentage of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_controversial_comments = df_spark.filter('controversiality = 1').count()\n",
    "n_controversial_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perc_controversial_comments = 100 * n_controversial_comments / total_n_comments\n",
    "perc_controversial_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having easily computed this percentages, we now get a dataframe with the number of comments per subrredit in order to plot the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of subreddits:\n",
    "df_spark[['subreddit']].distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_count = df_spark.groupBy('subreddit').agg(count('*')).withColumnRenamed('count(1)', 'Number of comments')\n",
    "df_subreddit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_subreddit_count.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not analyzed the statistics above as they are merely there for us to find the right functions to use wth the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show the stats with a bigger dataset (10% of the 2016 reddit comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline we used is the following:\n",
    "1. We first read all of the data in the 2016 .bz2 compressed files and sampled 10% of it to run our querries with the cluster (`script/read_data_original.py`)\n",
    "2. We then sampled another 10% to obtain 1% of the 2016 reddit comments that we played with above (`scripts/read_data.py`)\n",
    "3. Once we found the correct form for our querries on the smaller dataset, we ran them with cluster on the larger dataset (`scripts/`) and saved them to smaller sized parquet files.\n",
    "4. We then imported these smaller parquet files into our local machine and read them and transformed them to pandas dataframes in order to plot the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above stats were obtained using `scripts/stats.py` with cluster. The stats were saved in a text file (`data/stats.txt`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of comments: 79998223  \n",
    "Number of deleted comments: 3414381  \n",
    "% of deleted comments: 4  \n",
    "Number of comments by removed users: 5706216  \n",
    "% comments by deleted users: 7  \n",
    "Number of controversial comments: 1770571  \n",
    "% of controversial comments: 2  \n",
    "Number of subreddits: 131028  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first decided to have a look at the number of querries per subreddit (`scripts/stats.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_count = spark.read.parquet(DATA_DIR + 'df_subreddit_count.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_count.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_count.toPandas().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot the 10 most 'popular' (commented) subbreddits. As they are the ones to give us the most information in our future calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tmp = df_subreddit_count.toPandas().sort_values(by = \"Number_of_comments\", ascending = False).iloc[:10,:]\n",
    "df_tmp.plot(kind='barh',x = \"subreddit\", y = \"Number_of_comments\",legend = None)\n",
    "plt.title('10 most popular (commented) subbreddits')\n",
    "plt.xlabel('number of comments');\n",
    "plt.ylabel('subreddits');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now 10 least popular subreddits, as they will potentially be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_subreddit_count.toPandas().sort_values(by = \"Number_of_comments\", ascending = True).iloc[:10,:]\n",
    "df_tmp.plot(kind='barh',x = \"subreddit\", y = \"Number_of_comments\", legend = None);\n",
    "plt.title('10 least popular subreddits')\n",
    "plt.xlabel('number of comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the last 10 all have exactly 1 comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to see how many subreddits have 1 comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_count_pd = df_subreddit_count.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subreddits_per_comment = df_subreddit_count_pd.groupby('Number_of_comments').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subreddits_per_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subreddits_per_comment.hist(bins = [1,10,100,1000]);\n",
    "plt.title('Histogram showing the comment frequecy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see a lot of the subreddits have 1 comment, hence removing these will reduce the size of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now analyze the controversiality of the comments (`script/stats.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_controversial_per_subr_count = spark.read.parquet(DATA_DIR + \"df_controversial_per_subr_count.parquet\")\n",
    "df_controversial_per_subr_count.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controversial_per_subr_count_pd = df_controversial_per_subr_count.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most controversial subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controversial_per_subr_count_pd.sort_values(by='N_controversial_comments',ascending=False).iloc[:10,:].plot(kind='barh',x='subreddit',y='N_controversial_comments',legend=None);\n",
    "plt.title('10 most controversial subreddits');\n",
    "plt.xlabel('Number of controversial comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 least controversial subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controversial_per_subr_count_pd.sort_values(by='N_controversial_comments',ascending=True).iloc[:10,:].plot(kind='barh',x='subreddit',y='N_controversial_comments',legend = None)\n",
    "plt.title('10 least controversial subreddits');\n",
    "plt.xlabel('number of controversial comments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controversial_per_subr_count_pd.groupby('N_controversial_comments').count().hist(bins=[1,10,100,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the graph for most popular subbreddits (comments and controversial comments), some of the most popular subreddits are also some of the most controversial ones (askreddit, worldnews, politics,...) Hence there might be a correlation between the number of comments and controversiality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr_contr_comm.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_contr_comm.corr(\"N_controversial_comments\",\"Number_of_comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is a correlation between popularity and controversiality, what could be a good factor for our analysis of the echo-chambing effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot a stacked bar chart to see the ratio of controversial to non controversial comments per subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc = spark.read.parquet(DATA_DIR + \"df_comments_per_month.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_pd = df_created_utc.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_pd = df_created_utc_pd.iloc[:12,:].merge(df_created_utc_pd.iloc[12:,:], right_on = 'count', left_on = 'count')\n",
    "df_created_utc_pd = df_created_utc_pd[['month_y','month_x','count']]\n",
    "df_created_utc_pd.columns = ['month_int','month_string','n_of_comments']\n",
    "df_created_utc_pd = df_created_utc_pd.sort_values(by='month_int',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_controversial = spark.read.parquet(DATA_DIR + \"df_created_utc_controversial.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_controversial_pd = df_created_utc_controversial.toPandas()\n",
    "df_created_utc_controversial_pd.sort_values(by=\"month\",ascending=True)\n",
    "df_created_utc_controversial_pd.columns = ['month_int','n_of_controversial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_non_controversial = spark.read.parquet(DATA_DIR + \"df_created_utc_non_controversial.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_non_controversial_pd = df_created_utc_non_controversial.toPandas()\n",
    "df_created_utc_non_controversial_pd = df_created_utc_non_controversial_pd.sort_values(by='month',ascending=True)\n",
    "df_created_utc_non_controversial_pd.columns = ['month_int','n_of_non_controversial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_pd = df_created_utc_pd.merge(df_created_utc_controversial_pd,left_on='month_int',right_on = 'month_int')\n",
    "df_created_utc_pd = df_created_utc_pd.merge(df_created_utc_non_controversial_pd,left_on='month_int',right_on = 'month_int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_created_utc_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_created_utc_pd[['n_of_controversial','n_of_non_controversial']].plot(kind='bar', stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for each month in 2016 the number of non controversial comments outweigh the number of controversial comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "As a final step wiithin milestone 2, below are our first steps with NLP, basically trying out the sugested libraries. Here's a reference with some more libraries https://elitedatascience.com/python-nlp-libraries\n",
    "\n",
    "To use this libraries in a simple way and in order to execute queries quickly we are going to study now just the comments of one of the subreddits of the sample, \"The_Donald\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = df_pd[df_pd[\"subreddit\"] == \"The_Donald\"].sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy allows us to find named entities, thus identying the topic(s) of a post or discussion. spaCy can be found here https://spacy.io/ with instructions for installing here https://spacy.io/usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the installation: Execute in the anaconda prompt:\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm #(With admin permissions)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this library we are going to use the \"Named Entity Recognition\" functionality:\n",
    "\n",
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this simple query, let's see which are the usuals named entities of the \"The_Donald\" subreddit comments. \n",
    "\n",
    "Note: We have done this grammatical analysis using the function npl.pipe, in order to make faster the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for doc in nlp.pipe(df_nlp[\"body\"].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=4):\n",
    "    if doc.is_parsed:\n",
    "        tags.append([ent.label_ for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [val for sublist in tags for val in sublist]  #From a list of lists to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_npl_pd = pd.DataFrame(tags, columns=[\"tags\"])\n",
    "tags_count = df_npl_pd.tags.value_counts().sort_values(ascending=False)\n",
    "tags_count.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the most popular tags in this subreddit are  ORG, PERSON, GPE, NORP and DATE. Below you can see the meaning of each tag:\n",
    "* PERSON\tPeople, including fictional.\n",
    "* NORP\tNationalities or religious or political groups.\n",
    "* FAC\tBuildings, airports, highways, bridges, etc.\n",
    "* ORG\tCompanies, agencies, institutions, etc.\n",
    "* GPE\tCountries, cities, states.\n",
    "* LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART\tTitles of books, songs, etc.\n",
    "* LAW\tNamed documents made into laws.\n",
    "* LANGUAGE\tAny named language.\n",
    "* DATE\tAbsolute or relative dates or periods.\n",
    "* TIME\tTimes smaller than a day.\n",
    "* PERCENT\tPercentage, including \"%\".\n",
    "* MONEY\tMonetary values, including unit.\n",
    "* QUANTITY\tMeasurements, as of weight or distance.\n",
    "* ORDINAL\t\"first\", \"second\", etc.\n",
    "* CARDINAL\tNumerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the appearances of each tag in the sample studied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_count = pd.DataFrame(tags_count).reset_index()\n",
    "df_tags_count.columns=[\"Tags\", \"Appearances\"]\n",
    "df_tags_count.set_index(\"Tags\")\n",
    "\n",
    "df_tags_count.plot(kind=\"barh\", x = \"Tags\", y = \"Appearances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "\n",
    "TextBlob allows for sentiment analysis, translation, and more\n",
    "\n",
    "TextBlob can be found here https://textblob.readthedocs.io/en/dev/ with installation instructions here https://textblob.readthedocs.io/en/dev/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the sentiment analysis this library offers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "```\n",
    " testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    " testimonial.sentiment\n",
    " Output: Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)\n",
    " testimonial.sentiment.polarity\n",
    "0.39166666666666666\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to see how is the sentiment analysis of the \"The_Donald\" subreddit comments. We have added two columns to the original data frame, one with the polarity measured and the other with the other with the subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['sentiment_polarity'] = df_nlp[\"body\"].apply(lambda com: TextBlob(str(com)).sentiment.polarity)\n",
    "df_nlp['sentiment_subjectivity'] = df_nlp[\"body\"].apply(lambda com: TextBlob(str(com)).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_nlp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the distribution of both polarity and subjectivity.   \n",
    "\n",
    "Reminder: The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nlp['sentiment_polarity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the polarity distribution we check that most of the comments usually are no so polarized. However there is a considerable amount of comments polarized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp['sentiment_subjectivity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the subjectivity distribution we also see that there are lots of comments which are somehow subjectives, although the most common is not being subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to do the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit_users = df_spark.select('subreddit','author','id').groupby('subreddit') \\\n",
    "                                .agg(collect_set('author').alias('users'),count('id').alias('number_of_posts')) \\\n",
    "                                .filter('number_of_posts > 1000') \\\n",
    "                                .orderBy(asc('subreddit'))\n",
    "\n",
    "df = df_subreddit_users.toPandas()  #NECCESARY FOR A NEXT QUERY\n",
    "\n",
    "adjacency_matrix = np.zeros([df.shape[0],df.shape[0]])\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[0]):\n",
    "        if i == j:\n",
    "            adjacency_matrix[i,j] = 0\n",
    "        else:\n",
    "            subb1_users = set(df.iloc[i]['users'])\n",
    "            subb2_users = set(df.iloc[j]['users'])\n",
    "            n_common_users = len(subb1_users.intersection(subb2_users))\n",
    "            adjacency_matrix[i,j] = n_common_users\n",
    "\n",
    "df_adj = pd.DataFrame(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = df[\"subreddit\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_adj.columns = subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj[\"subreddits\"] = df[\"subreddit\"]\n",
    "df_adj.set_index(\"subreddits\", inplace = True)\n",
    "df_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain now the edge list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subreddit_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subreddit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge_list = pd.DataFrame([[subreddit_list[0], subreddit_list[1], adjacency_matrix[0,1]]], columns = [\"Subreddit1\", \"Subreddit2\", \"Weight\"])\n",
    "n_subreddits = len(subreddit_list)\n",
    "\n",
    "for i in range(n_subreddits):\n",
    "    for j in range(i+1, n_subreddits):\n",
    "        df_tmp = pd.DataFrame([[subreddit_list[i], subreddit_list[j], adjacency_matrix[i,j]]] , columns = [\"Subreddit1\", \"Subreddit2\", \"Weight\"])\n",
    "        edge_list = edge_list.append(df_tmp, ignore_index=True)\n",
    "\n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list[\"Weight\"].mean()\n",
    "#edge_list[\"Weight\"] = edge_list[\"Weight\"].apply(lambda x: 0 if x < 15 else x) #Removing soft links for seeing a better graph (weights to 0)\n",
    "edge_list.drop(edge_list[edge_list.Weight < 9].index, inplace = True) #Removing soft links for seeing a better graph (remove edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.set_index(\"Subreddit1\", inplace = True) #In order to not export the indexes to the .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list[\"Weight\"] = edge_list[\"Weight\"] * 100 #Too do a clearly visualization\n",
    "edge_list.to_csv(DATA_DIR + 'edge_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_parquet(DATA_DIR + \"topics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "political_subreddits_1 = df_topics[df_topics[\"topic\"] == \"politics\"][\"subreddit\"].tolist()\n",
    "edge_list_politics = edge_list.loc[(edge_list.index.isin(political_subreddits_1)) & edge_list['Subreddit2'].isin(political_subreddits_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge_list_politics.to_csv(DATA_DIR + 'edge_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see what are the subreddits which are more connected with other subreddits (whose users post also in other subreddits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_centralized = pd.DataFrame(index = subreddit_list, columns = ['Centralized_value']) \n",
    "for i in range(n_subreddits):\n",
    "    centralized_value = 0\n",
    "    for j in range(i+1, n_subreddits):\n",
    "        centralized_value = centralized_value + adjacency_matrix[i,j]\n",
    "    df_centralized.loc[subreddit_list[i]] = centralized_value\n",
    "    \n",
    "df_centralized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centralized.sort_values('Centralized_value', ascending = False).head(30).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using igraph library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import igraph\n",
    "import cairo \n",
    "from igraph import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = df_adj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(A)):                  #Removing soft links for seeing a better graph\n",
    "    for j in range(len(A[i])):\n",
    "        if A[i,j]<5:\n",
    "            A[i,j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_all_subreddits = igraph.Graph.Adjacency((A > 0).tolist())\n",
    "g_all_subreddits = Graph()\n",
    "g_all_subreddits = g_all_subreddits.Adjacency((A > 0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edge weights and node labels.\n",
    "g_all_subreddits.es['weight'] = A[A.nonzero()]\n",
    "g_all_subreddits.vs['label'] = subreddit_list  # or a.index/a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g_all_subreddits.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(g_all_subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layout = g_all_subreddits.layout_fruchterman_reingold()\n",
    "plot(g_all_subreddits, layout = layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the list of politic subreddits that we have in the current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_politics.reset_index().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge_list_politics.reset_index(inplace = True)\n",
    "politic_subreddits = edge_list_politics['Subreddit1'].values.tolist()\n",
    "politic_subreddits= politic_subreddits + (edge_list_politics['Subreddit2'].values.tolist())\n",
    "politic_subreddits = list(set(politic_subreddits))\n",
    "politic_subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics_adj = df_adj.loc[df_adj.index.isin(politic_subreddits)]\n",
    "politic_subreddits = df_politics_adj.index.unique().tolist()  #For get the list in the correct order\n",
    "df_politics_adj= df_politics_adj[politic_subreddits].copy()\n",
    "df_politics_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = df_politics_adj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_politic_subreddits = igraph.Graph.Adjacency((A > 0).tolist())\n",
    "g_politic_subreddits = Graph()\n",
    "\n",
    "to_delete_ids = [v.index for v in g_politic_subreddits.vs]\n",
    "g_politic_subreddits.delete_vertices(to_delete_ids)\n",
    "\n",
    "g_politic_subreddits = g_politic_subreddits.Adjacency((A > 0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add edge weights and node labels.\n",
    "g_politic_subreddits.es['weight'] = A[A.nonzero()]\n",
    "g_politic_subreddits.vs['label'] = politic_subreddits\n",
    "g_politic_subreddits.es['widht'] = A[A.nonzero()] * 0.01\n",
    "\n",
    "#What this algorithm (fruchterman_reingold) do is printing the nodes which have less weight connection more distanciated\n",
    "layout = g_politic_subreddits.layout_fruchterman_reingold(weights = g_politic_subreddits.es['weight'])  \n",
    "plot(g_politic_subreddits, layout = layout, edge_width = g_politic_subreddits.es['widht'], vertex_label = politic_subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_total_users = df.set_index(\"subreddit\")\n",
    "\n",
    "df_politics_adj_percentages = df_politics_adj.copy()\n",
    "for i in df_politics_adj_percentages.index:\n",
    "    total_users = len(df_for_total_users.loc[i].users)\n",
    "    df_politics_adj_percentages.loc[i] = df_politics_adj_percentages.loc[i] / total_users * 100\n",
    "    #print(i)\n",
    "    #print(df_politics_adj_percentages.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_perc = df_politics_adj_percentages.values\n",
    "g_politic_subreddits_2 = Graph().Adjacency((A_perc > 0).tolist())\n",
    "# Add edge weights and node labels.\n",
    "g_politic_subreddits_2.es['weight'] = A[A.nonzero()]\n",
    "g_politic_subreddits_2.vs['label'] = politic_subreddits\n",
    "g_politic_subreddits_2.es['widht'] = A_perc[A_perc.nonzero()] *0.3\n",
    "\n",
    "#What this algorithm (fruchterman_reingold) do is printing the nodes which have less weight connection more distanciated\n",
    "layout = g_politic_subreddits_2.layout_fruchterman_reingold(weights = g_politic_subreddits_2.es['weight'])  \n",
    "plot(g_politic_subreddits_2, layout = layout, edge_width = g_politic_subreddits_2.es['widht'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_politic_subreddits.vs['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to try to classify the users ideology. Assuming that every of these subreddits have their own \"personality\" and ideology, we are going to determine which of them represents better the ideology of every user. For doing that we are going to see in which of the subreddits the user has more score (meaning this that the user opinion is well or bad accepted by the general opinion of the subreddit) and by the number of comments posted in each subreddit (it is normal to thing that people who usually post in an specific subreddit is close to the way of thinking of general people in this subreddit; else this user could be revealed by the score of his comments!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain first a data frame with all the neccesary data. First we extract a list with all the users which have posted commments in some of these subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df.set_index(\"subreddit\")\n",
    "df_politic_subreddits = df_total.loc[politic_subreddits]\n",
    "\n",
    "users_list = []\n",
    "for u in df_politic_subreddits[\"users\"]:\n",
    "    users_list = users_list + u\n",
    "    \n",
    "users_list = list(set(users_list)) #Drop duplicates\n",
    "len(users_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have to classify every user of this list. The idea is to for every user and for each political subreddit determine a value which means the grade of belonging to it; finally we classify the user according to the gratest value. If there is a tie we add this user to both(or more) kinds of user:\n",
    "\n",
    "\n",
    "Note: Every comment posted in a certain subreddit add +1 point for belonging to it. The score is just added (do not mater if it is positive or negative). With this way of classification we are giving more importance to the score (in case there are great values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_classifier = pd.DataFrame(index = politic_subreddits)\n",
    "df_classifier[\"Users_list\"] = [list() for x in range(len(df_classifier.index))]\n",
    "\n",
    "#We use the original data frame df_pd in order to get the score and the number of comments in each political subreddit!!\n",
    "df_classifying = df_pd.loc[df_pd[\"subreddit\"].isin(politic_subreddits)][[\"author\", \"score\", \"subreddit\"]]\n",
    "for u in users_list:\n",
    "    subreddit_values = np.zeros(len(politic_subreddits))\n",
    "    df_user_comments = df_classifying[df_classifying[\"author\"] == u]\n",
    "    for index, row in df_user_comments.iterrows():\n",
    "        value = 1\n",
    "        value = value + row.score\n",
    "        sreddit_index = politic_subreddits.index(row.subreddit)\n",
    "        subreddit_values[sreddit_index] = subreddit_values[sreddit_index] + value\n",
    "    greatest_value = subreddit_values.max()\n",
    "    for i in range(len(subreddit_values)):         #Neccesary in case there is a tie\n",
    "        if(greatest_value > 0 and subreddit_values[i] == greatest_value):\n",
    "            df_classifier[\"Users_list\"].loc[politic_subreddits[i]] = df_classifier[\"Users_list\"].loc[politic_subreddits[i]] + [u] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classifier[\"Number_of_Users_Belonging\"] = [len(i) for i in df_classifier[\"Users_list\"]]\n",
    "df_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classification done, it is time to do the last graph of this kind:\n",
    "\n",
    "First we need a new adjacency matrix, where the value in the position subreddit1-subreddit2 will mean the quantity of users who are from the subreddit1-ideology and who have posted in the subreddit2. (For compute this matrix we use the dataframe `df` obtained at the beginning of this section, which have the list of users who have posted in each subreddit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"subreddit\", inplace = True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol_final = df_politics_adj.copy()\n",
    "for i in df_pol_final.index:\n",
    "    for j in df_pol_final.columns:\n",
    "        if i == j:\n",
    "            df_pol_final.loc[i][j]=0\n",
    "        else:\n",
    "            df_pol_final.loc[i][j] = len(list(set(df_classifier.loc[i][\"Users_list\"]) & set(df.loc[j][\"users\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics_adj_classified = df_pol_final\n",
    "for i in df_politics_adj_classified.index:\n",
    "    total_users = df_classifier.loc[i][\"Number_of_Users_Belonging\"]\n",
    "    df_politics_adj_classified.loc[i] = df_politics_adj_classified.loc[i] / total_users * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics_adj_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_classified = df_politics_adj_classified.values\n",
    "g_politic_subreddits_3 = Graph().Adjacency((A_classified > 0).tolist())\n",
    "# Add edge weights and node labels.\n",
    "g_politic_subreddits_3.es['weight'] = A_classified[A_classified.nonzero()]\n",
    "g_politic_subreddits_3.vs['label'] = politic_subreddits\n",
    "g_politic_subreddits_3.es['widht'] = A_classified[A_classified.nonzero()] *0.3\n",
    "\n",
    "v_size = [df_classifier.loc[s][\"Number_of_Users_Belonging\"] * 0.009 for s in politic_subreddits]\n",
    "g_politic_subreddits_3.es['vertex_size'] = v_size \n",
    "\n",
    "#What this algorithm (fruchterman_reingold) do is printing the nodes which have less weight connection more distanciated\n",
    "layout = g_politic_subreddits_3.layout_fruchterman_reingold(weights = g_politic_subreddits_3.es['weight'])  \n",
    "plot(g_politic_subreddits_3, layout = layout, edge_width = g_politic_subreddits_3.es['widht'], vertex_size = v_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC DETECTION - TRYED: Classifying users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_for_training = [\"The_Donald\", \"politics\"]\n",
    "republicans_for_training = [\"The_Donald\"]\n",
    "democrats_for_training = [\"politics\"]\n",
    "\n",
    "df_training = (df_pd.loc[df_pd['subreddit'].isin(subreddits_for_training)])[[\"subreddit\", \"body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[\"Conservative\"] = df_pd['subreddit'].isin(republicans_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[\"Conservative\"] = df_training[\"Conservative\"].apply(lambda x: 1 if x == True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[df_training[\"Conservative\"]==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = df_training[\"body\"].values\n",
    "comments_list_class = df_training[\"Conservative\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "processed_docs = list()\n",
    "for doc in nlp.pipe(comments_list, n_threads=5, batch_size=10):\n",
    "\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams too\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from gensim.models import LdaMulticore\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model = LdaMulticore(corpus=corpus, num_topics=2, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(model[corpus[0]],key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topics\n",
    "data =  pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment\n",
    "sent_to_cluster = list()\n",
    "for n,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        cluster = max(model[doc],key=lambda x:x[1])\n",
    "        sent_to_cluster.append(cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from collections import Counter\n",
    "for book, cluster in book_id.items():\n",
    "    assignments = list()\n",
    "    for real,given in zip(comments_list_class,sent_to_cluster):\n",
    "        if real == cluster:\n",
    "            assignments.append(given)\n",
    "    most_common,num_most_common = Counter(assignments).most_common(1)[0] # 4, 6 times\n",
    "    print(book,\":\",most_common,\"-\",num_most_common)\n",
    "    print(\"Accuracy:\",num_most_common/limit)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
