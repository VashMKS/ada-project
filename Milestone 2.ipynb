{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Echo Chamber Effect - A Case Study (provisional title)\n",
    "\n",
    "## Milestone 2\n",
    "\n",
    "In this notebook we will download a small sample of the Reddit dataset for the first time and perform some basic statistics on it.\n",
    "\n",
    "We will also load the two recommended NLP libraries and try them out to see how they work and to which extent we can take advantage of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "#findspark.init(r'C:\\Users\\jorge\\Anaconda3\\pkgs\\pyspark-2.3.1-py36_1001\\Lib\\site-packages\\pyspark')\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a tiny slice of the 2017 data from the cluster, compressed it into a parquet file and downloaded to our local machines in order to mess with it. If succesful we may try with a bigger and more representative sample (e.g. with data from all subreddits and all years sampled randomly but maintaining relative size bewteen subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = spark.read.parquet(DATA_DIR + 'sample2017.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº of subreddits\n",
    "# relative sizes\n",
    "# some graphs with posting frequency\n",
    "# nº of posts per user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP\n",
    "\n",
    "Below are our first steps with NLP, basically trying out libraries. Here's a reference with some more libraries https://elitedatascience.com/python-nlp-libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy\n",
    "\n",
    "spaCy allows us to find named entities, thus identying the topic(s) of a post or discussion.\n",
    "\n",
    "spaCy can be found here https://spacy.io/ with instructions for installing here https://spacy.io/usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process whole documents\n",
    "text = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        u\"Google in 2007, few people outside of the company took him \"\n",
    "        u\"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        u\"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        u\"worth talking to,” said Thrun, now the co-founder and CEO of \"\n",
    "        u\"online higher education startup Udacity, in an interview with \"\n",
    "        u\"Recode earlier this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Determine semantic similarities\n",
    "doc1 = nlp(u\"my fries were super gross\")\n",
    "doc2 = nlp(u\"such disgusting fries\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(doc1.text, doc2.text, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextBlob\n",
    "\n",
    "TextBlob allows for sentiment analysis, translation, and more\n",
    "\n",
    "TextBlob can be found here https://textblob.readthedocs.io/en/dev/ with installation istructions here https://textblob.readthedocs.io/en/dev/install.html\n",
    "\n",
    "Unfortunately, TextBlob is only available for MacOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "blob.tags           # [('The', 'DT'), ('titular', 'JJ'),\n",
    "                    #  ('threat', 'NN'), ('of', 'IN'), ...]\n",
    "\n",
    "blob.noun_phrases   # WordList(['titular threat', 'blob',\n",
    "                    #            'ultimate movie monster',\n",
    "                    #            'amoeba-like mass', ...])\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)\n",
    "# 0.060\n",
    "# -0.341\n",
    "\n",
    "blob.translate(to=\"es\")  # 'La amenaza titular de The Blob...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
